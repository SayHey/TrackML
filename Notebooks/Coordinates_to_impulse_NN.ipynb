{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Coordinate to impulse space Neural Network** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebbok I will attempt to create Neural Network that learns transformation from coordinate to impulse space. \n",
    "\n",
    "Theoretically it should work well for particles that were born in origin.\n",
    "\n",
    "Here is the main notebook where I train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event\n",
    "\n",
    "# Import keras\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Lambda\n",
    "from keras.initializers import TruncatedNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "data_path = '../Data/generated_data/'\n",
    "x_train_df = pd.read_csv(os.path.join(data_path, \"Coordinates_to_impulse_X.csv\"))\n",
    "y_train_df = pd.read_csv(os.path.join(data_path, \"Coordinates_to_impulse_Y.csv\"))\n",
    "x_train = np.transpose(np.asarray([x_train_df[\"0\"], x_train_df[\"1\"], x_train_df[\"2\"]]))\n",
    "y_train = np.transpose(np.asarray([y_train_df[\"0\"], y_train_df[\"1\"], y_train_df[\"2\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024.6  1024.73 2955.5 ]\n"
     ]
    }
   ],
   "source": [
    "max_coordinates = np.amax(x_train, axis=0)\n",
    "print(max_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 16.695  11.169 150.701]\n"
     ]
    }
   ],
   "source": [
    "max_impulses = np.amax(y_train, axis=0)\n",
    "print(max_impulses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhance data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ganerate polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(3,  include_bias=False)\n",
    "x_train_poly = poly.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_scaler = StandardScaler().fit(x_train_poly)\n",
    "x_train_poly = x_scaler.transform(x_train_poly) \n",
    "\n",
    "y_scaler = StandardScaler().fit(y_train)\n",
    "y_train_scaled = y_scaler.transform(y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -553.203    39.266 -1215.5  ]\n",
      "[-2.27   0.152 -1.122  1.743 -0.235  1.823 -0.415 -0.129  0.18  -1.425\n",
      "  0.225 -1.577 -0.009  0.206 -0.963 -0.012  0.001  0.076 -0.316]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1])\n",
    "print(x_train_poly[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.144  0.177 -0.429]\n",
      "[-0.173  0.239 -0.043]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[1])\n",
    "print(y_train_scaled[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def model_architecture():\n",
    "    model = Sequential()\n",
    "    \n",
    "    #initializer = TruncatedNormal(stddev=0.1)\n",
    "    \n",
    "    # Normalization layer\n",
    "    # model.add(Lambda(lambda x: x / 1000))\n",
    "    \n",
    "    model.add(Dense(10000, activation='tanh'))\n",
    "    \n",
    "    model.add(Dense(1))    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, Y, learning_rate = 0.001, batch_size=1000, epochs=100):   \n",
    "    # Compiling the model\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    # Running and evaluating the model\n",
    "    history = model.fit(X, Y,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model\n",
    "model_check = model_architecture()\n",
    "\n",
    "#sanity check data\n",
    "y_check = np.linalg.norm(x_train, axis=1)\n",
    "#y_check = np.sum(x_train, axis=1)\n",
    "\n",
    "#model sanity check\n",
    "train_model(model_check, x_train_poly, y_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_check = model_check.predict(x_train_poly[0:5], batch_size=5)\n",
    "print(predict_check[0:5])\n",
    "print(y_check[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model for px\n",
    "model_px = train_model(x_train, y_train[:,0], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model for px\n",
    "model_py = train_model(x_train, y_train[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model for px\n",
    "model_pz = train_model(x_train, y_train[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = 10\n",
    "np.set_printoptions(precision=3, suppress = True)\n",
    "predict_px = model_px.predict(x_train[0:lim], batch_size=lim)\n",
    "predict_py = model_py.predict(x_train[0:lim], batch_size=lim)\n",
    "predict_pz = model_pz.predict(x_train[0:lim], batch_size=lim)\n",
    "predict = np.concatenate((predict_px, predict_py, predict_pz), axis=1)\n",
    "print(predict[0:lim])\n",
    "print(y_train[0:lim])\n",
    "#print(x_train[0:lim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def predict(X, eps=0.01):\n",
    "    cl = DBSCAN(eps, min_samples=1, algorithm='kd_tree')\n",
    "    labels = cl.fit_predict(StandardScaler().fit_transform(X))\n",
    "    return labels\n",
    "\n",
    "#score\n",
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n",
    "\n",
    "def try_clustering(hits, X):\n",
    "    labels = predict(X)\n",
    "    submission = create_one_event_submission(0, hits, labels)\n",
    "    score = score_event(truth, submission)\n",
    "    print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Data/train_100_events/'\n",
    "event_prefix = 'event000001000'\n",
    "hits, cells, particles, truth = load_event(os.path.join(data_path, event_prefix))\n",
    "\n",
    "X = np.transpose([hits.x.values, hits.y.values, hits.z.values])\n",
    "\n",
    "y_predict = model.predict(X, batch_size=10000)\n",
    "\n",
    "try_clustering(hits, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
