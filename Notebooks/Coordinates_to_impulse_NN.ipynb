{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Coordinate to impulse space Neural Network** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebbok I will attempt to create Neural Network that learns transformation from coordinate to impulse space. Theoretically it should work well for particles that were born in origin.\n",
    "\n",
    "First I'm going to implement basic NN that learns on entire data set.\n",
    "Then I'll remove non origin particles from training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "data_path = '../Data/train_100_events/'\n",
    "event_prefix = 'event000001000'\n",
    "hits, cells, particles, truth = load_event(os.path.join(data_path, event_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "hits = shuffle(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [] # np.transpose([hits.x.values, hits.y.values, hits.z.values])\n",
    "y_train = []\n",
    "progress = 0\n",
    "\n",
    "for index, hit in hits.iterrows():\n",
    "    hit_id = hit['hit_id']\n",
    "    truth_of_hit = truth.loc[truth.hit_id == hit_id]\n",
    "    particle_id = truth_of_hit['particle_id'].values[0]    \n",
    "    if particle_id == 0: continue\n",
    "        \n",
    "    particle_info = particles[particles.particle_id == particle_id]\n",
    "    charge = particle_info['q'].values[0]    \n",
    "    if charge > 0: continue\n",
    "        \n",
    "    origin = [particle_info['vx'].values[0],\n",
    "              particle_info['vy'].values[0]]\n",
    "    \n",
    "    if np.linalg.norm(origin) > 0.1: continue\n",
    "        \n",
    "    coordinates = [hit['x'], hit['y'], hit['z']]\n",
    "    impulse = [particle_info['px'].values[0],\n",
    "               particle_info['py'].values[0],\n",
    "               particle_info['pz'].values[0]]\n",
    "    \n",
    "    x_train.append(coordinates)\n",
    "    y_train.append(impulse)\n",
    "    \n",
    "    if len(x_train)%1000 == 0 : \n",
    "        progress+=1\n",
    "        print(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "print(x_train[0:10])\n",
    "print(y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_coordinates = np.amax(x_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Lambda\n",
    "from keras.initializers import TruncatedNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def model_architecture():\n",
    "    model = Sequential()\n",
    "    \n",
    "    initializer = TruncatedNormal(stddev=0.1)\n",
    "    \n",
    "    # Normalization layer\n",
    "    model.add(Lambda(lambda x: x / 1000))\n",
    "    \n",
    "    model.add(Dense(100, kernel_initializer=initializer, activation='relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(1000, kernel_initializer=initializer, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(3, kernel_initializer=initializer))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model\n",
    "model = model_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model using categorical_crossentropy loss, and rmsprop optimizer.\n",
    "from keras.optimizers import Adam\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='mse',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running and evaluating the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=1000,\n",
    "                    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = 10\n",
    "y_predict = model.predict(x_train[0:lim], batch_size=lim)\n",
    "print(x_train[0:lim])\n",
    "print(y_predict[0:lim])\n",
    "print(y_train[0:lim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def predict(X, eps=0.5):\n",
    "    cl = DBSCAN(eps, min_samples=1, algorithm='kd_tree')\n",
    "    labels = cl.fit_predict(StandardScaler().fit_transform(X))\n",
    "    return labels\n",
    "\n",
    "#score\n",
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n",
    "\n",
    "def try_clustering(hits, X):\n",
    "    labels = predict(X)\n",
    "    submission = create_one_event_submission(0, hits, labels)\n",
    "    score = score_event(truth, submission)\n",
    "    print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.transpose([hits.x.values, hits.y.values, hits.z.values])\n",
    "\n",
    "y_predict = model.predict(X, batch_size=10000)\n",
    "\n",
    "try_clustering(hits, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO tomorrow\n",
    "---\n",
    "Ok so now I have to separate regression into 3 different networks.\n",
    "Also adjust data generation, may be generate csv with coordinates to impulse mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
